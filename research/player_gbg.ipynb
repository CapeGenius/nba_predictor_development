{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_players_df = pd.read_csv(\"data/all_players.csv\")\n",
    "all_games_df = pd.read_csv(\"data/all_games.csv\",encoding=\"utf-8\")\n",
    "columns = [\"FG_PCT\", \"FG3_PCT\", \"FTM\", \"OREB\", \"DREB\", \"REB\", \"AST\"]\n",
    "string_columns = [\"GAME_DATE\", \"GAME_ID\"]\n",
    "columns_a = [column + \"_A\" for column in columns]\n",
    "columns_b = [column + \"_B\" for column in columns]\n",
    "games_df = all_games_df[string_columns + columns_a + columns_b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_players_df.drop(\n",
    "    columns=[\n",
    "        \"HomeTeamPlayer1_PLAYER_NAME\",\n",
    "        \"HomeTeamPlayer2_PLAYER_NAME\",\n",
    "        \"HomeTeamPlayer3_PLAYER_NAME\",\n",
    "        \"HomeTeamPlayer4_PLAYER_NAME\",\n",
    "        \"HomeTeamPlayer5_PLAYER_NAME\",\n",
    "        \"HomeTeamPlayer6_PLAYER_NAME\",\n",
    "    ], inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_players_df.drop(\n",
    "    columns=[\n",
    "        \"AwayTeamPlayer1_PLAYER_NAME\",\n",
    "        \"AwayTeamPlayer2_PLAYER_NAME\",\n",
    "        \"AwayTeamPlayer3_PLAYER_NAME\",\n",
    "        \"AwayTeamPlayer4_PLAYER_NAME\",\n",
    "        \"AwayTeamPlayer5_PLAYER_NAME\",\n",
    "        \"AwayTeamPlayer6_PLAYER_NAME\",\n",
    "    ],\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gets Necessary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(all_players_df.columns)\n",
    "home_1 = columns.index(\"TEAM_ID_home\")\n",
    "home_2 = columns.index(\"TEAM_ID_away\")\n",
    "away_1 = columns.index(\"TEAM_ID_away\")\n",
    "home_columns = columns[home_1:home_2]\n",
    "away_columns = columns[away_1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merges Last N Game Averages with Data on All Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from api_helpers.game_stats_helpers import load_player_n_games\n",
    "\n",
    "# gets the last n games for players\n",
    "players_n_df = load_player_n_games(\n",
    "    players_df=all_players_df, home_columns=home_columns, away_columns=away_columns, n=5\n",
    ")\n",
    "\n",
    "# merges data on the player's average for the past n games with all games\n",
    "merged_data = pd.merge(\n",
    "    players_n_df, games_df, on=[\"GAME_ID\", \"GAME_DATE\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_337257/1351649269.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  classifier_data[\"WL_A\"].replace(\"W\", 1, inplace=True)\n",
      "/tmp/ipykernel_337257/1351649269.py:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  classifier_data[\"WL_A\"].replace(\"L\", 0, inplace=True)\n",
      "/tmp/ipykernel_337257/1351649269.py:14: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  classifier_data[\"WL_A\"].replace(\"L\", 0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "from api_helpers.game_stats_helpers import load_player_n_games\n",
    "\n",
    "classifier_columns = [\"GAME_ID\", \"GAME_DATE\", \"WL_A\"]\n",
    "\n",
    "\n",
    "# merges data on the player's average for the past n games with all games\n",
    "classifier_data = pd.merge(\n",
    "    players_n_df,\n",
    "    all_games_df[classifier_columns],\n",
    "    on=[\"GAME_ID\", \"GAME_DATE\"],\n",
    ")\n",
    "\n",
    "classifier_data[\"WL_A\"].replace(\"W\", 1, inplace=True)\n",
    "classifier_data[\"WL_A\"].replace(\"L\", 0, inplace=True)\n",
    "classifier_data[\"WL_A\"].dropna(inplace=True)\n",
    "\n",
    "classifier_data.to_csv(\"data/n_players_classifier.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import keras_tuner as kt\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(\n",
    "    \"data/n_players_classifier.csv\"\n",
    ")\n",
    "\n",
    "# Replace win/loss indicators with binary values\n",
    "df[\"TEAM_1_WIN/LOSS\"] = df[\"TEAM_1_WIN/LOSS\"].replace({100: 1, 0: 0})\n",
    "\n",
    "# Drop rows where the target variable contains NaN\n",
    "df = df.dropna(subset=[\"TEAM_1_WIN/LOSS\"])\n",
    "\n",
    "# Define features and target\n",
    "X = df[\n",
    "    [\n",
    "        \"TEAM_1_HOME/AWAY\",\n",
    "        \"TEAM_1_DEF_PPP\",\n",
    "        \"TEAM_1_TS%\",\n",
    "        \"TEAM_1_eFG%\",\n",
    "        \"TEAM_1_FG_PCT\",\n",
    "        \"TEAM_1_DREB\",\n",
    "        \"TEAM_1_AST\",\n",
    "        \"TEAM_1_TOV\",\n",
    "        \"TEAM_1_WIN_PCT\",\n",
    "        \"TEAM_2_HOME/AWAY\",\n",
    "        \"TEAM_2_DEF_PPP\",\n",
    "        \"TEAM_2_TS%\",\n",
    "        \"TEAM_2_eFG%\",\n",
    "        \"TEAM_2_FG_PCT\",\n",
    "        \"TEAM_2_DREB\",\n",
    "        \"TEAM_2_AST\",\n",
    "        \"TEAM_2_TOV\",\n",
    "        \"TEAM_2_WIN_PCT\",\n",
    "    ]\n",
    "]\n",
    "y = df[\"TEAM_1_WIN/LOSS\"]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# One-hot encode the target variable\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# Apply SMOTE to balance the classes\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "# Define the hypermodel with Mixture Density Network\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Dense(\n",
    "            units=hp.Int(\"units\", min_value=16, max_value=128, step=16),\n",
    "            activation=hp.Choice(\"activation\", values=[\"relu\", \"tanh\"]),\n",
    "            input_shape=(X_train.shape[1],),\n",
    "        )\n",
    "    )\n",
    "    model.add(Dropout(hp.Float(\"dropout\", min_value=0.0, max_value=0.5, step=0.1)))\n",
    "    model.add(\n",
    "        Dense(\n",
    "            units=hp.Int(\"units\", min_value=16, max_value=128, step=16),\n",
    "            activation=hp.Choice(\"activation\", values=[\"relu\", \"tanh\"]),\n",
    "        )\n",
    "    )\n",
    "    model.add(Dropout(hp.Float(\"dropout\", min_value=0.0, max_value=0.5, step=0.1)))\n",
    "\n",
    "    # Define the parameters for the mixture density network\n",
    "    num_components = hp.Int(\"num_components\", min_value=2, max_value=10, step=1)\n",
    "    model.add(\n",
    "        tf.keras.layers.Dense(num_components * 3)\n",
    "    )  # 3 parameters per component: mean, stddev, and weight\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Custom loss function for Mixture Density Network\n",
    "def mdn_loss(num_components):\n",
    "    def loss(y_true, y_pred):\n",
    "        y_true = tf.expand_dims(y_true, axis=-1)\n",
    "        gm = tfp.distributions.MixtureSameFamily(\n",
    "            mixture_distribution=tfp.distributions.Categorical(\n",
    "                logits=y_pred[:, :num_components]\n",
    "            ),\n",
    "            components_distribution=tfp.distributions.Normal(\n",
    "                loc=y_pred[:, num_components : num_components * 2],\n",
    "                scale=tf.nn.softplus(y_pred[:, num_components * 2 :]),\n",
    "            ),\n",
    "        )\n",
    "        return -gm.log_prob(y_true)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Initialize the tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective=\"val_loss\",\n",
    "    max_trials=3,\n",
    "    executions_per_trial=2,\n",
    "    directory=\"my_dir\",\n",
    "    project_name=\"nba_mdn_tuning\",\n",
    ")\n",
    "\n",
    "# Search for the best hyperparameters\n",
    "tuner.search(X_train_resampled, y_train_resampled, epochs=50, validation_split=0.2)\n",
    "\n",
    "# Get the best model\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Compile the model with the custom loss\n",
    "num_components = best_hps.get(\"num_components\")\n",
    "model.compile(optimizer=\"adam\", loss=mdn_loss(num_components))  # Default optimizer\n",
    "\n",
    "# Train the best model\n",
    "history = model.fit(\n",
    "    X_train_resampled, y_train_resampled, epochs=50, validation_split=0.2, verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Best Mixture Density Network loss: {loss}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Convert predictions to categorical outputs for evaluation\n",
    "gm = tfp.distributions.MixtureSameFamily(\n",
    "    mixture_distribution=tfp.distributions.Categorical(\n",
    "        logits=y_pred[:, :num_components]\n",
    "    ),\n",
    "    components_distribution=tfp.distributions.Normal(\n",
    "        loc=y_pred[:, num_components : num_components * 2],\n",
    "        scale=tf.nn.softplus(y_pred[:, num_components * 2 :]),\n",
    "    ),\n",
    ")\n",
    "y_pred_proba = gm.mean().numpy()  # Convert Tensor to NumPy array\n",
    "\n",
    "# Ensure exactly half wins and half losses\n",
    "num_samples = len(y_pred_proba)\n",
    "num_wins = num_samples // 2\n",
    "num_losses = num_samples - num_wins\n",
    "\n",
    "# Get indices for the top probabilities for wins\n",
    "indices_sorted = np.argsort(y_pred_proba.flatten())\n",
    "indices_wins = indices_sorted[-num_wins:]\n",
    "indices_losses = indices_sorted[:-num_wins]\n",
    "\n",
    "# Create the final predictions\n",
    "y_pred_final = np.zeros(num_samples)\n",
    "y_pred_final[indices_wins] = 1  # Mark top probabilities as wins\n",
    "# Remaining are losses\n",
    "\n",
    "# Convert y_test from one-hot encoded format to single integer labels\n",
    "y_test_single = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Best Mixture Density Network classification report:\")\n",
    "print(classification_report(y_test_single, y_pred_final))\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix_mdn = confusion_matrix(y_test_single, y_pred_final)\n",
    "sns.heatmap(conf_matrix_mdn, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
